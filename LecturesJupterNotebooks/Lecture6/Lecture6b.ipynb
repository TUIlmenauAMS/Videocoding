{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\" color =\"Brown\"><center>\n",
    "# Lecture 6, Video Coding, Motion Compensation\n",
    "        \n",
    "<center></font>\n",
    "<br>\n",
    "\n",
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">\n",
    "    \n",
    "Last time we saw 2 error measures for the motion estimation. We now have an optimization task: minimize the error with our motion vectors. Mathematically we can write this task as\n",
    "\n",
    "$$ { \\binom{{v_{min} = argmin}}{  v} } ⁡  {{MSE  (  v  )} }$$\n",
    "\n",
    "<br>\n",
    "where $v_{min}$ is our motion vector. “argmin” means we minimize the MSE with v, and the result is the argument at which the MSE becomes the minimum.<br>\n",
    "How do we **search** the image for the best motion vector $v_{min}$? A simple, but computationally inefficient way would be to simply scan a search area (it can be an area smaller than the entire picture to reduce complexity) in raster order,<br><br>\n",
    "![Lecture6b-1.PNG](Img-Lecture6b\\Lecture6b-1.PNG)\n",
    "<br><br>\n",
    "\n",
    "If we are searching for motion vectors with pixel accuracy, we need to advance our search block pixel-wise. This is also called **full search** motion estimation, which leads to a relatively **high computational complexity**.<br> \n",
    "To **reduce complexity**, we can make the assumption that the motion vector is close to the origin with high probability (small motion). Hence we can modify this search approach by starting in the origin and have a spiral wise search path. We can then stop this search if we have some stopping criteria (e.g. reaching a local minimum or a certain threshold of the error),<br>\n",
    "\n",
    "![Lecture6b-2.PNG](Img-Lecture6b\\Lecture6b-2.PNG)\n",
    "<br>\n",
    "\n",
    "This is a spiral order. But this approach has the problem that we can easily get stuck in a **local minimum**. To avoid this problem we can apply a hierarchical search order, for instance the so-called three step search (see Richardson, Video Codec Design), where we start with a coarse search of our search area, and then refine our search near the minimum of the coarser search. We can do this in 3 steps,<br>\n",
    "\n",
    "![Lecture6b-3.PNG](Img-Lecture6b\\Lecture6b-3.PNG)\n",
    "<br>\n",
    "\n",
    "The hierarchical three step search starts with a coarse search, which means we advance the block by more than one pixel, in the example by several pixels. In this way we get a coarse motion vector to the minimum of this first step, with an accuracy of a few pixels. In the second step, we set this minimum as the new center of our search, and obtain a motion vector with increased accuracy (for instance 2 pixels). In the final step we further reduce our search area and search with final precision (for instance 1 pixel accuracy).<br><br>\n",
    "Observe that this approach assumes, that the error function is more or less smooth over the search area, which is often true for natural images.<br><br>\n",
    "\n",
    "An often used approach to reduce the search complexity is the so-called nearest neighbor search. It uses the assumption, that motion vectors don’t change much between neighboring blocks. Here we chose as start of our search first the origin (no movement) and then the previous motion vector,<br>\n",
    "\n",
    "![Lecture6b-4.PNG](Img-Lecture6b\\Lecture6b-4.PNG)\n",
    "<br>\n",
    "\n",
    "Another fast efficient approach to find the motion vectors is to **use the FFT** to compute a correlation function between shifted blocks, see e.g.:<br> \n",
    "S. Drew Kilthau and T. M. S. Moller, “Full Search Content Independent Block Matching Based on the Fast Fourier Transform”,  International Conference on Image Processing (ICIP) 2002, pp. I–669– I–672. <br><br>\n",
    "\n",
    "In usual coders we transmit the found motion vectors to the decoder, as **side information**. In this way, the motion vectors are based on the current frame and the previous frame, and have the highest accuracy. The transmission of the motion vectors needs bit rate, but usually this is more than compensated for by the savings we get from better prediction and hence smaller difference values. In principle we could avoid sending motion vectors if we only base them on past frames, for instance the last frame and the frame before the last frame. In this case the decoder could perform the same operations to obtain the same motion vectors as the encoder, **without transmitting** them. But then the motion vectors would be **less accurate**, because they are not based on the current frame but on the previous frame. It would only work sufficiently if the motion does not change much.\n",
    "\n",
    "</font></p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\" color =\"Brown\"><center>\n",
    "## Python Example\n",
    "        \n",
    "<center></font>\n",
    "<br>\n",
    "\n",
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">The following shows a python example for motion estimation with a full search in the range of +/- 8 pixels around the current block (which is a rather small range):<br>\n",
    "\n",
    "```python videoencframewkYCrCb420mc.py```\n",
    "<br>\n",
    "</font></p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480 640\n",
      "Frame no  0\n",
      "Frame no  1\n",
      "Frame no  2\n",
      "Frame no  3\n",
      "Frame no  4\n",
      "Frame no  5\n",
      "Frame no  6\n",
      "Frame no  7\n",
      "Frame no  8\n",
      "Frame no  9\n",
      "Frame no  10\n",
      "Frame no  11\n",
      "Frame no  12\n",
      "Frame no  13\n",
      "Frame no  14\n",
      "Frame no  15\n",
      "Frame no  16\n",
      "Frame no  17\n",
      "Frame no  18\n",
      "Frame no  19\n",
      "Frame no  20\n",
      "Frame no  21\n",
      "Frame no  22\n",
      "Frame no  23\n",
      "Frame no  24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys \n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "   # for Python 2\n",
    "   import cPickle as pickle\n",
    "else:\n",
    "   # for Python 3\n",
    "   import pickle\n",
    "\n",
    "import scipy.signal\n",
    "\n",
    "#Program to capture video from a camera and store it in an recording file, in Python txt format, using cPickle\n",
    "# Also, filed is divided into luminance and two color components. By using 4:2:0 scheme, color components are then downsampled by the factor of 2. To reduce aliasing artifacts pyramidial filter is used.\n",
    "#With motion estimation and display of motion vectors\n",
    "#This is a framework for a simple video encoder to build.\n",
    "#It writes into file 'videorecord.txt'\n",
    "#Gerald Schuller, May 2015\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "N=2\n",
    "g=open('videorecord.txt', 'wb')\n",
    "# filter is created by convolving two rectangular filters\n",
    "#Triangular filter kernel:\n",
    "filt1=np.ones((N,N))/N;\n",
    "filt2=scipy.signal.convolve2d(filt1,filt1)/N\n",
    "\n",
    "#Get size of frame:\n",
    "[retval, frame] = cap.read()\n",
    "[rows,columns,d]=frame.shape\n",
    "print(rows,columns)\n",
    "#Grid of 8x8 blocks:\n",
    "gc=np.zeros((1,columns))\n",
    "gc[0,0:columns:8]=np.ones(int(columns/8))\n",
    "gr=np.zeros((rows,1))\n",
    "gr[0:rows:8,0]=np.ones(int(rows/8))\n",
    "grid=np.ones((rows,1))*gc+gr*np.ones((1,columns))\n",
    "grid3=np.zeros((rows,columns,3))\n",
    "grid3[:,:,1]=grid\n",
    "#print(grid[0:9,0:9])\n",
    "\n",
    "\n",
    "#Prevous Y frame:\n",
    "Yprev=np.zeros((rows,columns))\n",
    "#Vectors for current frame as graphic:\n",
    "framevectors=np.zeros((rows,columns,3))\n",
    "#motion vectors, for each block a 2-d vector:\n",
    "mv=np.zeros((int(rows/8),int(columns/8),2))\n",
    "\n",
    "#Process 25 frames:\n",
    "for n in range(25):\n",
    "    print(\"Frame no \",n)\n",
    "    ret, frame = cap.read()\n",
    "    [rows,columns,c]=frame.shape\n",
    " \n",
    "    if ret==True:\n",
    "       \n",
    "        #Here goes the processing to reduce data... \n",
    "        reduced = np.zeros((rows,columns,c))\n",
    "        Y=(0.114*frame[:,:,0]+0.587*frame[:,:,1]+0.299*frame[:,:,2]);        \n",
    "        \n",
    "        Cb=(0.4997*frame[:,:,0]-0.33107*frame[:,:,1]-0.16864*frame[:,:,2]);\n",
    "   \n",
    "        Cr=(-0.081282*frame[:,:,0]-0.418531*frame[:,:,1]+0.499813*frame[:,:,2]);\n",
    "        reduced[:,:,0]=Y\n",
    "        reduced[:,:,1]=Cb\n",
    "        reduced[:,:,2]=Cr\n",
    "       \n",
    "        #print(grid3.shape)\n",
    "        #print(framevectors.shape)\n",
    "\n",
    "        cv2.imshow('Original',frame/255.0+framevectors)\n",
    "        #cv2.imshow('Luminanz Y',Y[0:200,0:100]/255)\n",
    "        #cv2.imshow('Unterabgetastetes Y',Ds)\n",
    "        #cv2.imshow('Farbkomponente U',np.abs(Cr))\n",
    "        #cv2.imshow('Farbkomponente V',np.abs(Cb))\n",
    "       \n",
    "   \n",
    "        #Two color components are filtered first\n",
    "        Crfilt=scipy.signal.convolve2d(Cr,filt2,mode='same')\n",
    "        Cbfilt=scipy.signal.convolve2d(Cb,filt2,mode='same')\n",
    "\n",
    "        # Downsampling\n",
    "        DCr=Crfilt[0::N,::N];\n",
    "        DCb=Cbfilt[0::N,::N];\n",
    "        #cv2.imshow('Crfiltered',DCr)\n",
    "        #cv2.imshow('Cbfiltered',DCb)\n",
    "\n",
    "        #Motion estimation, correlate current Y block with previous 16x16 block which contains current 8x8 block:\n",
    "        #Start pixel for block wise motion estimation:\n",
    "        block=np.array([8,8])\n",
    "        framevectors=np.zeros((rows,columns,3))\n",
    "        #for loops for the blocks:\n",
    "        #print(\"for loops for the motion vectors:\")\n",
    "        for yblock in range(5):\n",
    "           #print(\"yblock=\",yblock)\n",
    "           block[0]=yblock*8+200;\n",
    "           for xblock in range(5):\n",
    "              #print(\"xblock=\",xblock)\n",
    "              block[1]=xblock*8+300;\n",
    "              #print(\"block= \", block)\n",
    "              #current block:\n",
    "              Yc=Y[block[0]:block[0]+8 ,block[1]:block[1]+8]\n",
    "              #print(\"Yc= \",Yc)\n",
    "              #previous block:\n",
    "              #Yp=Yprev[block[0]-4 : block[0]+12 ,block[1]-4 : block[1]+12]\n",
    "              #print(\"Yp= \",Yp)\n",
    "              #correlate both to find motion vector\n",
    "              #print(\"Yp=\",Yp)\n",
    "              #print(Yc.shape)\n",
    "              #Some high value for MAE for initialization:\n",
    "              bestmae=10000.0;\n",
    "              #For loops for the motion vector, full search at +-8 integer pixels:\n",
    "              for ymv in range(-8,8):\n",
    "                 for xmv in range(-8,8):\n",
    "                    diff=Yc-Yprev[block[0]+ymv : block[0]+ymv+8, block[1]+xmv: block[1]+xmv+8];\n",
    "                    mae=sum(sum(np.abs(diff)))/64;\n",
    "                    if mae< bestmae:\n",
    "                       bestmae=mae;\n",
    "                       mv[yblock,xblock,0]=ymv;\n",
    "                       mv[yblock,xblock,1]=xmv;\n",
    "\n",
    "              #Ycorr=scipy.signal.correlate2d(Yp, Yc,mode='valid')\n",
    "              #print(\"Ycorr= \", Ycorr)\n",
    "              #motion vector:\n",
    "              #1-d Index of maximum\n",
    "              #index1d=np.argmax(Ycorr)\n",
    "              #print(\"inedx1d=\",index1d)\n",
    "              #convert it to 2d index:\n",
    "              #index2d=np.unravel_index(index1d,(9,9))\n",
    "              #print(\"arg max of correlation: \", index2d)\n",
    "              #2-d index minus center coordinates (4,4) is the motion vector\n",
    "              #print(\"mv=\",mv[0,0,:])\n",
    "              #print(np.subtract(index2d,(4,4)))\n",
    "              #mv[0,0,:]=np.subtract(index2d,(4,4))\n",
    "              #print(\"mv[0,0,:]=\",mv[0,0,:])\n",
    "              #print(tuple(np.add(block,mv[0,0,:]).astype(int)))\n",
    "              #cv2.line(framevectors, block, (block[0]+mv[0],block[1]+mv[1]),(1.0,1.0,1.0))\n",
    "              cv2.line(framevectors, (block[1], block[0]), (block[1]+mv[yblock,yblock,1].astype(int),block[0]+mv[yblock,yblock,0].astype(int)) , (1.0,1.0,1.0));\n",
    "        Yprev=Y.copy();\n",
    "        #converting  images back to integer:\n",
    "        Y=np.array(Y, dtype='uint8')\n",
    "        DCr=np.array(DCr, dtype='int8')\n",
    "        DCb=np.array(DCb, dtype='int8')\n",
    " #\"Serialize\" the captured video frame (convert it to a string) \n",
    "        #using pickle, and write/append it to file g:\n",
    "        pickle.dump(Y,g)\n",
    "        pickle.dump(DCr,g)\n",
    "        pickle.dump(DCb,g)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "g.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">\n",
    "Observe: only 5x5 current blocks are chosen for motion estimation, and it is still quite slow. This shows that the faster approaches above are really necessary.\n",
    "<br>\n",
    "</font></p>    \n",
    "\n",
    "\n",
    "<font size=\"8\" color =\"Brown\"><center>\n",
    "## Sub-Pixel Motion Estimation\n",
    "        \n",
    "<center></font>\n",
    "<br>\n",
    "\n",
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">\n",
    "    \n",
    "Recent video coding standards get much of their improved compression from sub-pixel motion estimation. The basic approach is to increase the size of our search image using interpolation between pixels, and then search for the motion vector in this bigger image. For instance, if we want half pixel accuracy, we need to increase the number of pixels by 2, each horizontally and vertically (for a total factor of 4).<br><br>\n",
    "An often used interpolation is the **“bi-linear”** interpolation. It computes the new pixel value simply as the **weighted average** of the values of the neighboring pixel values. The weighting is done using    <br>\n",
    "![Lecture6b-5.PNG](Img-Lecture6b\\Lecture6b-5.PNG)\n",
    "<br>\n",
    "Hence, if the new pixel is in the middle between two old pixels, the new pixel value (e.g. brightness) is simply the average of the values of the 2 neighbouring values. Or if the new pixel is in the middle (center) of 4 old pixels, its value is the average of those 4 pixel values. If the new pixel is closer to one of the old pixels, its value is the weighted average of the neighboring pixels, with an accordingly **higher weight for the closer pixel**.<br><br>\n",
    "**Example:**\n",
    "(from: http://en.wikipedia.org/wiki/Bilinear_interpolation)\n",
    "\n",
    "\n",
    "![Lecture6b-6.PNG](Img-Lecture6b\\Lecture6b-6.PNG)\n",
    "<br>\n",
    "</font></p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
