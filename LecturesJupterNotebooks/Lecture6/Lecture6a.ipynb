{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\" color =\"Brown\"><center>\n",
    "# Lecture 6, Video Coding, Compression Approaches\n",
    "        \n",
    "<center></font>\n",
    "<br>\n",
    "\n",
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">\n",
    "What are more methods to use redundancy to compress our image or video? For redundancy reduction we need a source model. A common source model for natural images is, that they have many even surfaces, meaning that there is a high correlation between neighbouring pixels.<br>\n",
    "A simple method to use this correlation between neighbouring pixels is predictive coding. One example is the so-called DPCM, or Differential Pulse Code Modulation.<br>\n",
    "\n",
    "![Lecture6a-1.PNG](Img-Lecture6a\\Lecture6a-1.PNG)    \n",
    "<br>\n",
    "Here we would like to predict the current pixel as accurate as possible. We then transmit only the difference of the actual pixel value to the predicted pixel value. If the prediction is Saccurate, and the signal is predictable, we will get many small prediction error values. Since we have a high probability for those small values, we assign short code words to them, and leave the long code words to the less probable higher values (entropy coding). In this way we obtain a reduced bit rate.<br><br>\n",
    "An example for a very simple prediction would be to use the pixel just before, to the left (A) as predicted value.  A slightly more complex method would be to compute the weighted average of the values of pixels A,B, and C, for the predicted value. We don’t need to limit ourselves to pixels A,B, and C, we also can take a larger neighbourhood into account.<br><br>\n",
    "This usually gives a modest compression with low complexity (Richardson, Video Codec Design), and is also called “intra frame prediction”.<br>\n",
    "\n",
    "</font></p>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\" color =\"Brown\"><center>\n",
    "## Motion Estimation and Prediction\n",
    "        \n",
    "<center></font>\n",
    "<br>\n",
    "\n",
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">\n",
    "    \n",
    "We can extend this approach from 2-dimensional images to our 3-dimensional video sequences, where we now also have time as the additional dimension. We not only have neighbouring pixels in the spatial domain (x,y), but also in the temporal domain (pixels in previous and following frames). In this way we simply extend our neighbourhood of pixels, and could again apply a weighted sum to obtain our prediction.<br><br>\n",
    "This would also work reasonably well, but we can improve our temporal prediction by the model assumption that we have constant (not changing) objects in our video sequence, which move smoothly.\n",
    "To use this model assumption, we search the picture for displacements (movements), and estimate motion vectors for the image. These motion vectors then increase the accuracy of the prediction over the time dimension. <br><br>\n",
    "The prediction error is then computed based on the motion compensated predicted image. The encoder then sends the **motion vectors** and the **prediction error** (the so-called **residual frame**) to the decoder. The decoder can compute the new frame using the motion vectors, the previous frame(s) (and perhaps also future frames, if we also compute motion vectors based on future frames), plus the residual frame, to compute the current frame.<br> The following image shows this basic approach:<br><br>\n",
    "\n",
    "![Lecture6a-2.PNG](Img-Lecture6a\\Lecture6a-2.PNG) \n",
    "\n",
    "<center>**Fig.2** Video CODEC with motion estimation and compensation\n",
    "(source: Richardson: “Video Codec Design”)\n",
    "<center>\n",
    "<br><br>\n",
    "\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">\n",
    "**-Motion Vector:** The vector which describes the motion of an image block<br>\n",
    "**-Motion Prediction:** The algorithm which computes the predicted image<br>\n",
    "**-Motion Compensation:** Computing the prediction error using motion prediction<br>\n",
    "**-Motion Estimation:** The estimation algorithm for estimating the motion vector\n",
    "<br><br>\n",
    "Observe the image **decoder in the video encoder**. It is necessary because only in this way the video decoder has the same basis for its prediction as the encoder. If the encoder would take its original frames as basis, its prediction would be different, and this would lead to the so-called **“drift”** between encoder and decoder, with increasing degradation of quality over time. This is especially important for long prediction sequences. For short prediction sequences, like if every $2^{nd}$ frame is a so-called Intra-predicted frame, which is not predicted from past frames, this might be less of a problem.<br><br>\n",
    "\n",
    "How do we **compute** those **motion vectors**? \n",
    "We can look for motion of individual pixels (if we take into account a neighbourhood of pixels, which then overlap), or blocks of pixels (for instance 8x8 blocks, which then don’t overlap), or complete objects.<br>\n",
    "What is used most often is block based motion estimation (usually 8x8 or 16x16 blocks). This means, for each of these consecutive, non-overlapping blocks in the current frame, we compute a motion vector, to see from which position the block came from in the previous frame, as can be seen in following picture:\n",
    "<br><br>\n",
    "![Lecture6a-3.PNG](Img-Lecture6a\\Lecture6a-3.PNG)    \n",
    "Observe that the motion vectors have a length and a direction. <br>\n",
    "Different standards use **different precision** for the motion vectors (pixel accuracy, sub-pixel accuracy..), which usually makes a big difference in **compression performances**. \n",
    "<br><br>\n",
    "\n",
    "So how do we **obtain** those vectors (with different accuracies)?\n",
    "<br><br>\n",
    "We take a block of the current image and shift it pixel wise or sub-pixel wise over a certain neighbourhood of the previous frame, and look for the biggest similarity or correlation between this shifted current block and the block in the position of the previous frame. <br>\n",
    "Observe that usually we only have **plain motion** in our model, not rotation or stretching or shrinking. This is a **restriction**, but it also simplifies our task of finding suitable motion vectors.<br><br>\n",
    "This means we also need a **similarity measure**. What is usually used is the **mean squared error** between the predicted (shifted previous) and the current block:<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "MSE=\\frac{1}{N^2}\\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1}(C_{i,j}-R_{i,j})^2\n",
    "\n",
    "$$\n",
    "\n",
    "<br>\n",
    "where C is the current block, R is the reference block (shifted “window” in previous block), and N is the block size.<br>\n",
    "Alternatively we can take the sum of the absolute differences (**Mean Absolute Error, MAE**): \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "MAE=\\frac{1}{N^2}\\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1}\\mid C_{i,j}-R_{i,j}\\mid\n",
    "\n",
    "$$\n",
    "<br>\n",
    "\n",
    "</font></p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
