{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ips0fhGMCLJ6"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture9/Lecture9.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "<font size=\"8\" color =\"Brown\"><center>\n",
        "Lecture 9 Video Coding\\\n",
        "Transforms 2      \n",
        "</center></font>\n",
        "<br>\n",
        "\n",
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "\n",
        "**Integer Transform of H.264/AVC**<br>\n",
        "In previous standards, the DCT was defined as the ideal transform, with **unlimited accuracy**. This has the problem, that we have encoders with **limited accuracy** for the DCT, and decoders, with different accuracy for the inverse DCT. This then leads to **reconstruction errors** from the different implementations. Also, if we want to implement the DCT with low error, we need an arithmetic with long word length and floating point implementation, which makes the implementation complex. This is a problem for low end processors, it makes the coder hardware more expensive. Since Video decoding plays an increasing role in consumer hardware, where cost is very important, this problem was addressed in H.264. They solved the problem by specifying the **DCT with integer arithmetic**, such that the rounding errors we make are now known (since it is specified in the standard). In this way, we can still obtain **perfect reconstruction** (in the absence of quantization), because in the standard we can specify the **inverse** DCT with **integer arithmetic**, such that we obtain the exact inverse. In this way, the rounding errors „fit“ to each other, to obtain perfect reconstruction. In this way we get a different transform, which is not quite the DCT, but only similar to it. <br>\n",
        "The approach was to take the DCT matrix, **multiply** it with a **certain factor**, and **round** it to obtain a transform matrix with integer entries. A factor which turned out to be suitable is $\\alpha=2.5$ (see also: H. Malvar et. Al: „Low Complexity Transform and Quantization in H.264/AVC“, IEEE Trans. on Circuits and Systems for Video Technology, July 2003, to find under: **ieeexplore.ieee.org** inside the University Net). It still leads to a good coding gain, but allows a low wordlength processing (16 bit arithmetic).<br><br>\n",
        "H.264 uses 8x8 and also 4x4 transform matrices. Here we look at the 4x4 version. We start with the usual 4x4 DCT type 2 (shown are 4 digits after the decimal point),<br>\n",
        "</font></p>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEZFkhPYCLJ9"
      },
      "source": [
        "\n",
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"5\">\n",
        "$$\\boldsymbol T= \\left[\\matrix { 0.5000 &   0.5000   & 0.5000   & 0.5000 \\\\\n",
        "    0.6533&0.2706&-0.2706&-0.6533 \\\\\n",
        "    0.5000&-0.5000& -0.5000&0.5000 \\\\\n",
        "    0.2706&-0.6533&0.6533&-0.2706\n",
        " } \\right]$$\n",
        "    \n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYiCE8cVCLJ9"
      },
      "source": [
        "\n",
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "Here we assume that we multiply our signal vector **x** from the right hand side, to obtain the transformed signal: $\\boldsymbol y^T= \\boldsymbol T.\\boldsymbol x^T$ , and for the inverse: $\\boldsymbol x^T=T^{-1}.\\boldsymbol y^T$.\n",
        "With the factor of $\\alpha=2.5$ we get    \n",
        "    \n",
        "</font></p>     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjIxnEEuCLJ9"
      },
      "source": [
        "\n",
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "$$\n",
        "    2.5 \\cdot \\boldsymbol T  = \\left[ \\matrix{\n",
        "    1.2500  &  1.2500 &   1.2500 &   1.2500 \\\\\n",
        "    1.6332  &  0.6765 &  -0.6765 &  -1.6332 \\\\\n",
        "    1.2500  & -1.2500 &  -1.2500 &   1.2500 \\\\\n",
        "    0.6765  &  -1.6332 &   1.6332 &  -0.6765} \\right]\n",
        "$$  \n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDFtS0bUCLJ-"
      },
      "source": [
        "<p style=\"line-height:1.5\"><br>\n",
        "<font size=\"6\">Now we can round it to obtain a useful transform matrix with integer entries,<br>\n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiLVlAqMCLJ-"
      },
      "source": [
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "$$\\boldsymbol H := round(2.5 \\cdot \\boldsymbol T)= \\left[ \\matrix{\n",
        "     1  &   1  &   1  &   1\\\\\n",
        "     2  &   1  &  -1  &  -2\\\\\n",
        "     1  &  -1  &  -1   &  1\\\\\n",
        "     1  &  -2  &  2   & -1}\\right]\n",
        "$$\n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJZmtd_jCLJ-"
      },
      "source": [
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "    \n",
        "This is the **forward transform matrix used by H.264**. We see that it is quite similar to the WHT, but gives better compression performance. Malvar writes that for a stationary Gauss-Markov input with correlation coefficient $\\rho = 0.9$ the coding gain for the DCT is 5.39 dB, whereas for this **integer** version it is 5.38 dB. <br>\n",
        "Generation of this **stationary Gauss-Markov input** y(n):<br>\n",
        "![Lecture9-1.PNG](https://github.com/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture9/Img-Lecture9/Lecture9-1.PNG?raw=1)\n",
        "or: $y( n )=x( n )+ 0.9 \\cdot y( n-1 )$, with<br>\n",
        "x(n): random memoryless gaussian input<br>\n",
        "y(n): filtered output, the stationary Gauss-Markov signal. In the z-domain we obtain a pole at z=0.9 in the transfer function, which makes it a low pass. This **fits to natural images**, because they also have a low pass characteristic. But also observe that this is only a very crude and simple approximation of a natural image, but at least this gives us a simple model of a natural image.<br>\n",
        "The **coding gain** is defined as the **arithmetic average divided by the geometric average** of the squares of the subband values, $y^{2}_k$, to give us an estimate of the compression performance of a given subband decomposition (in dB). The arithmetic average is our usual average as the sum of values divided by the number of values, the geometric average is using the product and the Nth root instead of the sum and the division by N:<br>\n",
        "\n",
        "$$\n",
        "\\text{coding-gain}:\\frac{\\frac{1} {N} \\cdot \\sum _{k=0} ^ {N-1} y^2_k} {{\\sqrt [N]{ \\prod _{k=0}^ {N-1} y^2_k}}}\n",
        "$$\n",
        "<br>\n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aILGT52TCLJ_"
      },
      "source": [
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">See also: Jayant, Noll: “Digital Coding of Waveforms”, Prentice Hall.<br>\n",
        "The **arithmetic average** is the energy or power of the signal. **Parsevals Theorem** says that the sum of the  power of the subband signals is identical to the signals power in the time or space domain. If we take the log2 of the arithmetic average above, we get an estimate of the needed **bits per sample in the time or space domain** (times 2 since we have the square), also called the Entropy.<br><br>\n",
        "If we take the log2 of the **geometric average**, we get an estimate of the number of needed **bits per sample** to encode the signal in the **subbands**.\n",
        "<br>\n",
        "$$\n",
        "    \\log_2(\\sqrt [N] {\\prod _{k=0} ^ {N-1} y^2_k})=\\sum_{k=0}^{N-1}  \\log_2(y^2_k)/N\n",
        "$$\n",
        "    \n",
        "Here, $\\log_2(y^2_k)$ is an estimated number of bits (times 2 because of the square) per sample in subband k, hence the sum is the estimate of the average number of bits per sample over all subbands (Entropy over the subbands).<br>\n",
        "So if we take the $log_2$ of the coding gain as defined above, we obtain the difference in needed bits (times 2) for the direct and the subband coding. If we take the 10*log10, we obtain the number in dB, where we have about **6dB per bit**. The usual is the **coding gain in dB**. We would like to have the coding gain as high as possible. <br>\n",
        "<br>\n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP29OfAfCLJ_"
      },
      "source": [
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "\n",
        "**Observe:** If the coding gain is 1, or 0 dB, all the subband values $y_k$ are identical. Only in this case, the geometric and arithmetic average become identical. The more different the $y_k$ become, the higher the coding gain, with the extreme of a $y_k$ being 0. In this case we have an infinite coding gain. If we have **6dB coding gain, we save 1 bit/sample**. See the Book by Jayant/Noll.\n",
        "<br><br>\n",
        "So at least for this artificial signal it is only a very small loss in coding gain of only 0.01 dB for the rounded DCT (or a cost of only 0.01/6 bits/sample) . For a WHT the loss in coding gain would be clearly higher. Observe that we can use the same approach (with the same factor) to obtain an integer 8x8 transform.<br>\n",
        "We still need an **integer** value **inverse**, for the decoder. Is it possible to get an integer valued **exact** inverse?<br>\n",
        "We can simply first compute the inverse of **H**,<br>\n",
        "\n",
        "\n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL0IcC5FCLJ_"
      },
      "source": [
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "$$\n",
        "\\boldsymbol H ^{-1}= \\left[ \\matrix{\n",
        "    0.2500  &  0.2000  &  0.2500 &   0.1000\\\\\n",
        "    0.2500  &  0.1000 &  -0.2500  & -0.2000\\\\\n",
        "    0.2500  & -0.1000 &  -0.2500 &   0.2000 \\\\\n",
        "    0.2500 &  -0.2000 &   0.2500 &  -0.1000}\\right]\n",
        "$$\n",
        "    \n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH4O7drpCLJ_"
      },
      "source": [
        "\n",
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "    \n",
        "\n",
        "Here we could use again a factor to obtain integer values for the inverse transform. In this way we would obtain the original, but scaled with the two factors. But the goal is to have **factors as small as possible** to have lower **wordlengths** for the integer arithmetic. The trick used here is to apply **different factors** to the columns, and allow factors which can be implemented with a shift operation (e.g. 0.5). These factors are 4;5;4;5. <br>With these factors we get the matrix<br>\n",
        "\n",
        "    \n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muva1QrvCLKA"
      },
      "source": [
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "$$\n",
        "\\boldsymbol {\\tilde H}_{inv} =\\left[ \\matrix{\n",
        "1 &1& 1& 1/2\\\\\n",
        "1 &1/2& -1&-1\\\\\n",
        "1 & -1/2&-1& 1\\\\\n",
        "1 & -1& 1& -1/2}\\right]\n",
        "$$\n",
        "\n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5EOhgIYCLKA"
      },
      "source": [
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "    \n",
        "    \n",
        "This means we get the inverse as<br><br>\n",
        "$$\\boldsymbol H ^{-1}  = \\boldsymbol {\\tilde H}_{inv} \\cdot  diag(1/4,1/5,1/4,1/5) $$\n",
        "This shows that we just extracted the diagonal matrix from the inverse. The diagonal matrix does not need to be computed explicitly, because we can factor it into the inverse quantization of the decoder.<br>\n",
        "So, in this way we obtained the exact inverse with very simple numbers or fractions, easy to implement!<br><br>\n",
        "A fast implementation:<br>\n",
        "![Lecture9-2.PNG](https://github.com/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture9/Img-Lecture9/Lecture9-2.png?raw=1)\n",
        "<br>\n",
        "(From [1]).\n",
        "</font></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAN-HafMCLKA"
      },
      "source": [
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "    \n",
        "**Dynamic range of values after the transform:**<br>\n",
        "(important for the fixed wordlength integer arithmetic). Assume we have an input signal with a maximum value of A, for instance an image with  brightness levels A (for the worst case this would be the maximum value). Then we have a signal vector containing the values +-A (for instance for the chrominance values, which can also be negative), which is here multiplied from the right hand side. If we take the transform matrix **H** from last time, and if one column of **x** has for instance is the vector $[A,A,-A,-A]^T$ (as a worst case again) then the multiplication with the second row of H results to **6A**.  This would also be the maximum value for this matrix. If we then also transform the rows of our image, we get a maximum value of $6*6A= 36A$. This means that the dynamic range we have for our subband coefficients increases by $\\log_2(36)=5.17$ bits compared to the dynamic range of the original images (we would need 5.17 bits or rather 6 bits more for our fixed wordlength integer arithmetic). This is an overhead which we need to provide in our coding signal processor. This is also the reason why we wanted to have our factor $\\alpha$ as small as possible.<br><br>\n",
        "For the inverse matrix, in the decoder, the factor is somewhat smaller. Here we get a factor of 4, leading to a factor of 16 for rows and colums, and hence 4 additional bits for the dynamic range for the decoded subband samples. Observe that this also means a reduced (maximum) information content in the subband signals, which is the result of the quantization in the encoder.<br>\n",
        "These effects become important if we want to implement our algorithm with integer arithmetic, with limited word length. H.264 is made such that it can be implemented with **16 bit** arithemtic (in encoder and decoder, and which is sufficient, as we see here), which enables the implementation into cheap hardware.\n",
        "    \n",
        "</font></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIAF-PWtCLKA"
      },
      "source": [
        "\n",
        "<font size=\"8\" color =\"Brown\"><center>\n",
        "Literature:      \n",
        "</center></font>\n",
        "<br>\n",
        "\n",
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "- Jayant, Noll: “Digital Coding of Waveforms”, Prentice Hall.\n",
        "<br>\n",
        "<br>\n",
        "- H. Malvar et. Al: „Low Complexity Transform and Quantization in H.264/AVC“, IEEE Transactions on Circuits and Systems for Video Technology, July 2003\n",
        "<br>\n",
        "<br>\n",
        "-Kalva, H.: \"The H.264 Video Coding Standard \"<br>\n",
        "IEEE  Transactions on Multimedia, <br>\n",
        "Volume: 13 , Issue: 4 <br>\n",
        "Digital Object Identifier: 10.1109/MMUL.2006.93 <br>\n",
        "Publication Year: 2006 , Page(s): 86 - 90\n",
        "<br>\n",
        "<br>\n",
        "-Ugur, K.; Andersson, K.; Fuldseth, A.; Bjøntegaard, G.; Endresen, L.P.; Lainema, J.; Hallapuro, A.; Ridge, J.; Rusanovskyy, D.; Cixun Zhang; Norkin, A.; Priddle, C.; Rusert, T.; Samuelsson, J.; Sjöberg, R.; Zhuangfei Wu:<br>\n",
        "\"Low complexity video coding and the emerging HEVC standard\"<br>\n",
        "Picture Coding Symposium (PCS), 2010<br>\n",
        "Digital Object Identifier: 10.1109/PCS.2010.5702540<br>\n",
        "Publication Year: 2010 , Page(s): 474 - 477 <br>\n",
        "<br>\n",
        "- Ugur, K.; Andersson, K.; Fuldseth, A.; Bjontegaard, G.; Endresen, L.P.; Lainema, J.; Hallapuro, A.; Ridge, J.; Rusanovskyy, D.; Cixun Zhang; Norkin, A.; Priddle, C.; Rusert, T.; Samuelsson, J.; Sjoberg, R.; Zhuangfei Wu:<br>\n",
        "\"High Performance, Low Complexity Video Coding and the Emerging HEVC Standard \",<br>\n",
        "IEEE Transactions on Circuits and Systems for Video Technology,<br>\n",
        "Volume: 20 , Issue: 12<br>\n",
        "Digital Object Identifier: 10.1109/TCSVT.2010.2092613 <br>\n",
        "Publication Year: 2010 , Page(s): 1688 - 1697 <br>\n",
        "<br>\n",
        "- Marpe, D.; Schwarz, H.; Bosse, S.; Bross, B.; Helle, P.; Hinz, T.; Kirchhoffer, H.; Lakshman, H.; Nguyen, T.; Oudin, S.; Siekmann, M.; Suhring, K.; Winken, M.; Wiegand, T.:<br>\n",
        "\"Video Compression Using Nested Quadtree Structures, Leaf Merging, and Improved Techniques for Motion Representation and Entropy Coding \"<br>\n",
        "IEEE Transactions on Circuits and Systems for Video Technology,<br>\n",
        "Volume: 20 , Issue: 12<br>\n",
        "Digital Object Identifier: 10.1109/TCSVT.2010.2092615 <br>\n",
        "Publication Year: 2010 , Page(s): 1676 - 1687\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "(All to be found at **ieeexplore.ieee.org** from inside the TU Ilmenau Network)<br>\n",
        "\n",
        "</font></p>    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEkgl9ruCLKA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}