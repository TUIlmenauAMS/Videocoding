{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"8\" color =\"Brown\"><center>\n",
    "## Stereo Video Coding\n",
    "<center></font>\n",
    "<br>    \n",
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">  \n",
    "A further application of the tools we saw\n",
    "(particularly the motion compensation and\n",
    "prediction) is stereo video coding. Stereo video\n",
    "is used for creating a spatial impression, where\n",
    "each eye sees its own picture or video, and\n",
    "where the pictures between the eyes have\n",
    "slight **disparities** between them, which\n",
    "contain the **depth information**. The closer an\n",
    "object is to the eyes, the more disparity we\n",
    "have between the pictures for each eye. The\n",
    "brain uses this disparity to estimate the depth\n",
    "of an object.<br>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">\n",
    "The simplest approach would be to have **2\n",
    "separate video streams**, one for each eye,\n",
    "with the problem that we obtain twice the data\n",
    "rate.<br>\n",
    "This problem of increased data rate becomes\n",
    "even worse if we have more than the 2 views,\n",
    "also called multi view video. This is used for\n",
    "instance for auto-stereoscopic displays (without\n",
    "glasses). These displays produce different\n",
    "views to different angles from the display, such\n",
    "that each eye sees a different view, and such\n",
    "that if the head is moved, slightly different\n",
    "views are seen, which increases the 3-D effect.\n",
    "The number of **views usually is up to about\n",
    "a dozen**.<br></font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">\n",
    "Another common approach for Stereo Displays \n",
    "is the use of shutter glasses. They use the fact\n",
    "that modern monitors or displays can display\n",
    "100 pictures per second or more, such that we\n",
    "can display one image for the left eye, followed\n",
    "by an image for the right eye, separated by the\n",
    "shutter glasses, which opens each glass only\n",
    "when the image for the corresponding eye is\n",
    "displayed. But this again needs only 2 views.\n",
    "The multi-view approach would then even\n",
    "increase the problem with the bit rate, by\n",
    "multiplying the bit rate needed for a single\n",
    "video stream by the number of streams.<br><br>\n",
    "An interesting approach to reduce the bit rate\n",
    "can be seen in auto-stereoscopic displays,\n",
    "where there are approaches to generate\n",
    "multiple views from just a pair of views, such\n",
    "that we have reduced multi view video to\n",
    "stereo video. This is also an important\n",
    "approach, because often only 2 views are\n",
    "available as a video source.<br><br>\n",
    "Our Goal is now to use the redundancies\n",
    "between the stereo and multi-view videos to\n",
    "reduce the bit rate or to generate new views.\n",
    "The problem, which becomes especially\n",
    "apparent when looking at autostereoscopic\n",
    "displays, is that the new views, that we\n",
    "generate, might not have sufficient quality.\n",
    "Often, visually important information cannot be\n",
    "interpolated, for instance if new angles with \n",
    "new patterns appear in the image.\n",
    "The solution of this problem would be\n",
    "important, because for instance the auto\n",
    "stereoscopic display development would then\n",
    "be independent of the transmission of multiview\n",
    "video. It could all be based for instance on\n",
    "stereo video content.<br><br>\n",
    "Even stereo video content contains a lot of\n",
    "redundancies between the 2 views, which could\n",
    "be used to reduce the necessary data rate for\n",
    "the transmission.<br><br>\n",
    "The approach that is taken is to use **motion\n",
    "estimation and compensation** not only in\n",
    "the temporal direction, but also **between the\n",
    "left and right stereo video**, or between the\n",
    "multiple view video streams. This principle can\n",
    "be seen in the following image,<br>\n",
    "\n",
    "![Lecture15-1.PNG](Img-Lecture15\\Lecture15-1.PNG)    \n",
    "(From: \"Joint Prediction Algorithm and Architecture for\n",
    "Stereo Video Hybrid Coding Systems\",\n",
    "Li-Fu Ding, Shao-Yi Chien, and Liang-Gee Chen, IEEE\n",
    "TRANSACTIONS ON CIRCUITS AND SYSTEMS\n",
    "FOR VIDEO TECHNOLOGY, VOL. 16, NO. 11,\n",
    "NOVEMBER 2006)<br><br><br>\n",
    "Applying the principle of motion compensation\n",
    "to the disparity between views is now called\n",
    "\"Disparity Compensation\". This is a principle\n",
    "which was also defined within MPEG for multi\n",
    "view video sequences.<br>\n",
    "A possible coder structure using this principle\n",
    "can be seen in following image,<br>\n",
    "\n",
    "![Lecture15-2.PNG](Img-Lecture15\\Lecture15-2.PNG)\n",
    "<br><br>\n",
    "The principle of joint motion and disparity\n",
    "compensation is illustrated in the following\n",
    "image (both from \"Joint Prediction Algorithm\n",
    "and Architecture for Stereo Video Hybrid\n",
    "Coding Systems\",\n",
    "Li-Fu Ding, Shao-Yi Chien, and Liang-Gee Chen,\n",
    "IEEE TRANSACTIONS ON CIRCUITS AND\n",
    "SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 16,\n",
    "NO. 11, NOVEMBER 2006)<br><br>\n",
    "\n",
    "![Lecture15-3.PNG](Img-Lecture15\\Lecture15-3.PNG)\n",
    "(SW: Search Window, ME: Motion Estimation,\n",
    "DE: Dispatity Estimation).\n",
    "Interesting is the quantitiy of the use of the\n",
    "disparity estimation in a typical video\n",
    "sequence,<br><br>\n",
    "\n",
    "![Lecture15-4.PNG](Img-Lecture15\\Lecture15-4.PNG)\n",
    "(DE: Disparity Estimation, ME: Motion \n",
    "Estimation, Q: Quality factor, higher is better\n",
    "qualty and more bit rate).\n",
    "<br><br>\n",
    "Here it can be seen that still just the motion\n",
    "estimation is used most often. The following\n",
    "image shows, that particularly for moving\n",
    "objects the disparity estimation is beneficial.\n",
    "For instance if a moving object frees a part of\n",
    "the background, which is already seen in the\n",
    "other view, it can be predicted using the other\n",
    "picture, but not the past picture,\n",
    "![Lecture15-5.PNG](Img-Lecture15\\Lecture15-5.PNG)\n",
    "<br>\n",
    "(DV: Disparity Vector).\n",
    "The performance of the proposed system in the\n",
    "above source can be seen in following image,\n",
    "![Lecture15-6.PNG](Img-Lecture15\\Lecture15-6.PNG)\n",
    "(SP: Simple Profile, TSP: Temporal Scalabilty\n",
    "Profile, JPA: Joint Prediction Algorithm).<br><br>\n",
    "Here it can be seen that there is the possibility\n",
    "of **considerable bit rate savings using\n",
    "disparity compensation.**<br>\n",
    "\n",
    "</font></p>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\" color =\"Brown\">\n",
    "\n",
    "## Depth Map Coding        \n",
    "\n",
    "</font>\n",
    "<br>\n",
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">Depth Map Coding\n",
    "Another simple possibility of 3-D video coding\n",
    "is to use depth maps. Here, only 1 view is\n",
    "encoded, and other views are generated using\n",
    "the depth map. This depth map can have a\n",
    "relatively low spatial resolution, because the\n",
    "exact object boundaries can be obtained from\n",
    "the main video view. This depth map can then\n",
    "be encoded with relatively low bit rate. The\n",
    "disadvantage is, that the additional views are\n",
    "only approximated, because there cannot be\n",
    "any generation of additional information. Hence\n",
    "this leads to 3D views which usually don't have\n",
    "a high quality as in transmitting the separate\n",
    "views.<br>\n",
    "The following image shows an example of a\n",
    "depth map,<br>\n",
    "\n",
    "![Lecture15-7.PNG](Img-Lecture15\\Lecture15-7.PNG)\n",
    "(from: \"COMPRESSION AND TRANSMISSION OF\n",
    "DEPTH MAPS FOR IMAGE-BASED RENDERING\"\n",
    "Ravi Krishnamurthy, Bing-Bing Chai, Hai Tao,\n",
    "and Sriram Sethuraman, ICIP 2001)<br><br>\n",
    "Here, picture (a) and (b) are the left and right\n",
    "view, and (c) is the generated depth map of the\n",
    "2 views. In the depth map, bright means near\n",
    "and dark means further away. This can be seen\n",
    "as a low resolution video in itself.<br>\n",
    "As a result, for the transmission we need just\n",
    "one high resolution video, accompanied with \n",
    "the low resolution depth map video, which is\n",
    "then used to generate an artificial 3D video\n",
    "view. This is also an approach treated in MPEG,\n",
    "for a standardization.\n",
    "</font></p>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\" color =\"Brown\">\n",
    "## Blue-Ray 3D:     \n",
    "</font>\n",
    "<br>\n",
    "\n",
    "<p style=\"line-height:1.5\">\n",
    "<font size=\"6\">Here we usually have a standard video stream,\n",
    "but with the left and right views appearing in\n",
    "the video as the left and right half of the video\n",
    "image, or the lower and upper part. This means\n",
    "we have a reduced spatial resolution for 3-D\n",
    "videos, either horizontally or vertically, and we\n",
    "have no explicitly use of the redundancies\n",
    "between the 2 views.\n",
    "</font></p>    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
