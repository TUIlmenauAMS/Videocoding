{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy1A5GI3NPOB"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture12/Lecture12.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "<font size=\"8\" color =\"Brown\"><center>\n",
        "Lecture 12, Video Coding \\\n",
        "Quality Measurement\n",
        "</center></font>\n",
        "<br>\n",
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">Assume, we have a video codec. How do we evaluate this codec, or compare it with other codecs? Especially if we use irrelevance reduction (properties of the eye), the best would be a subjective test, where we have several test viewers look at images under controlled conditions, and have them evaluate the decoded images or videos. This is called subjective quality measurement. Especially for small impairments, it is helpful to give the test viewers a comparison to the original, uncoded video, as can be seen in the following picture, where there is a switch between the original and the decoded video.<br>\n",
        "    \n",
        "![Lecture12-1.PNG](https://github.com/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture12/Img-Lecture12/Lecture12-1.PNG?raw=1)\n",
        "**Fig.1** DSCQS testing system<br>\n",
        "(source: Richardson: “Video Codec Design”)<br>\n",
        "<br>\n",
        "Using this system, the test viewers then evaluate the quality according to a scale, as in following picture:<br>\n",
        "\n",
        "![Lecture12-2.PNG](https://github.com/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture12/Img-Lecture12/Lecture12-2.PNG?raw=1)\n",
        "**Fig.2** DSCQS rating form\n",
        "(source: Richardson: “Video Codec Design”)\n",
        "<br>   <br>\n",
        "Observe that we have items A and B. Often, one of the 2 is the original, and the other the item under test. In this way it can be tested if the test viewer recognized the original. A similar methodology can be found for perceptual testing of audio material. To measure small differences, it is useful to have trained test viewers.\n",
        "<br><br>\n",
        "This subjective measurement yields the most reliable results, because it involves the human eye, as the target for our application (as long as the eye is the intended viewer). The drawback of it is, that it is quite time consuming and expensive to conduct. To solve this problem, also objective measurements are used, measurements which can be done by using a computer or machine. The most well known and most widely used in the context of video coding is the so-called Peak-SNR or PSNR. If we compute the mean squared error between the original and the decoded video or image as MSE (the average of all squared differences of the pixels of original and decoded version), then the PSNR is computed as:    \n",
        " <br>\n",
        " $$PSNR= 10\\log_{10}(\\frac{(2^n-1)^2}{MSE})$$\n",
        " <br>\n",
        " where n is the number of bits per pixel value (usually 8). Hence $(2^n-1)$represents the peak value in the image (usually 255), and hence the name “Peak” SNR (observe that we conduct this calculation per channel, i.e. for R,G,B or Y,Cb,Cr). The PSNR has the advantage, compared to the SNR, that it is independent of the internal representation of the image, meaning the number of bits per pixel (the SNR would change with the number of pixels).\n",
        "</font></p>    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLqvrtr_NPOG"
      },
      "source": [
        "\n",
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">\n",
        "\n",
        "**Advantage:** Very easy to compute.<br>\n",
        "**Disadvantage:**<br>\n",
        "    a) We need the original (not always available)<br><br>\n",
        "    b) Does not correlate well with human vision (it is only useful for instance to compare different versions of an algorithm, because its basic artifacts would not change)\n",
        "The latter reason is the bigger problem, because quality comparison is its main objective. Consider for instance the following pictures:<br>\n",
        "![Lecture12-3.PNG](https://github.com/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture12/Img-Lecture12/Lecture12-3.PNG?raw=1)\n",
        "![Lecture12-4.PNG](https://github.com/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture12/Img-Lecture12/Lecture12-4.PNG?raw=1) \\\n",
        "**Fig.3** Quality comparison of images<br>\n",
        "<br>\n",
        "Observe that the image a) appears with a higher quality than image b), because there the faces are sharper. But the PSNR is higher for image b), saying that image b) should have a higher quality. This is because in image a) the larger background is unsharp. But the PSNR does not distinguish between areas which are important for the eye (faces), and areas which are unimportant (background).\n",
        "<br><br>\n",
        "For this reason there are works to improve objective (machine based) measurements to include properties of the eye, to make the resulting quality scores more similar to subjective measurements. One example is the so-called Structural Similarity Measurement (SSIM, or MSSIM, from Al Bovik et al.) which emphasises “structure”, meaning edges in images.<br>\n",
        "Its performance is illustrated in following images:<br>\n",
        "![Lecture12-5.PNG](https://github.com/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture12/Img-Lecture12/Lecture12-5.PNG?raw=1)\n",
        "<br>\n",
        "**Fig.4** Comparison of “Boat” images with different types of distortions, all with MSE = 210. (a) Original image (8 bits/pixel; cropped from 512 x 512 to 256 x 256 for visibility). (b) Contrast-stretched image, MSSIM = 0,9168.(c) Mean-shifted image, MSSIM = 0,9900. (d) JPEG compressed image, MSSIM = 0,6949. (e) Blurred image, MSSIM = 0,7052. (f) Salt-pepper impulsive noise contaminated image, MSSIM = 0,7748.<br>\n",
        "<br>\n",
        "(source: Wan, Bovik, Sheikh: “Image Quality Assessment: From Error Visibility to Structural Similarity”, IEEE Trans. on Image Processing, April 2004.)\n",
        "<br><br>\n",
        "\n",
        "\n",
        "</font></p>   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\"><b>More Image Quality Measures </b>\n",
        "<br>\n",
        "<br>\n",
        "There are more image quality measure, among them also neural network based measures. Examples are:\n",
        "VMAF, DISTS, LPIPS\n",
        "<br>\n",
        "For an overview, read:\n",
        "https://www.elecard.com/page/article_interpretation_of_metrics\n",
        "<br>\n",
        "For VMAF, read:\n",
        "https://de.wikipedia.org/wiki/Video_Multi-Method_Assessment_Fusion\n",
        "<br>\n",
        "For LPIS, read:\n",
        "https://github.com/richzhang/PerceptualSimilarity\n",
        "<br>\n",
        "For LPIPS: No distortion means value Zero.\n",
        "Observe that it needs a long time to install, due to the large number of coefficients for its neural network.\n",
        "</font></p>   "
      ],
      "metadata": {
        "id": "Y0VMLb2Jzwrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python example for using LPIPS:"
      ],
      "metadata": {
        "id": "HwzGx9g99acJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TUIlmenauAMS/Videocoding\n",
        "!pip install lpips\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import lpips\n",
        "\n",
        "def convtolpips(photo):\n",
        "    #takes photo from cv2 and converts it for LPIPS\n",
        "    photoRGB=photo[:,:,::-1] #convert from BGR to RGB\n",
        "    phototranspose=np.transpose(photoRGB, (2,0,1))\n",
        "    #primary color index first\n",
        "    photolpips=torch.from_numpy((\n",
        "    phototranspose/255).astype(np.float32))\n",
        "    return  photolpips\n",
        "\n",
        "photo=cv2.imread('/content/Videocoding/LecturesJupterNotebooks/IMGP1690.JPG');\n",
        "photodist=0.5*photo #less contrast distortion\n",
        "photolpips=convtolpips(photo)\n",
        "photodistlpips=convtolpips(photodist)\n",
        "\n",
        "loss_fn_alex = lpips.LPIPS(net='alex')\n",
        "#loss_fn_vgg = lpips.LPIPS(net='vgg')\n",
        "#loss_fn_vgg(photolpips, photodistlpips)\n",
        "loss=loss_fn_alex (photolpips, photodistlpips)\n",
        "print(\"LPIPS loss= \", loss)"
      ],
      "metadata": {
        "id": "9-rybPL_9O88",
        "outputId": "ad3db133-cf77-4837-8962-8b5ca62ec022",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Videocoding' already exists and is not an empty directory.\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.0->lpips) (12.5.40)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "LPIPS loss=  tensor([[[[0.1260]]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0WTEcc_NPOG"
      },
      "source": [
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">**Other literature:**<br>\n",
        "- IEEE Transactions on Image Processing, vol. 9, np. 4, pp. 636-650, Apr. 2000<br>\n",
        "“Image Quality Assessment Based on a Degradation Model”<br>\n",
        "Niranjan Damera-Venkata, and Thomas D. Kite, Wilson S. Geisler, Brian L. Evans, and Alan C. Bovik.\n",
        "<br>\n",
        "- IEEE International Conference on Acoustics, Speech, & Signal Processing, May 2002<br>\n",
        "“Why is Image Quality Assessment So Difficult?”<br>\n",
        "Zhou Wang1, Alan C. Bovik1 and Ligang Lu2.<br>\n",
        "<br>\n",
        "The area of improved objective measurements is still an active research area. All available on ieeexplore.ieee.org within the network of the TU Ilmenau! (Try it for the  paper of Al Bovic above).\n",
        "<br>\n",
        "Future coders often deliver higher resolution at lower bit rates, meaning we have fewer bits per pixel. This means we need more sophisticated models of vision (at least implied) in those coders to obtain a good perceptual quality. This means we also need more sophisticated models of vision for our objective measurement systems.\n",
        "    \n",
        "</font></p>   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUy06-MlNPOH"
      },
      "source": [
        "\n",
        "<font size=\"8\" color =\"Brown\"><center>\n",
        "Standards for Representing Digital Video\n",
        "</center></font>\n",
        "<br>\n",
        "\n",
        "<p style=\"line-height:1.5\">\n",
        "<font size=\"6\">The first digital standards where derived from the analog TV standards (from: Richardson, “Video Codec Design”):\n",
        "<br><br>\n",
        "**Table 1** ITU-R BT.601-5 parameters<br>\n",
        "\n",
        "![Lecture12-6.PNG](https://github.com/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture12/Img-Lecture12/Lecture12-6.PNG?raw=1)\n",
        "This then leads to the following standards:<br><br>    \n",
        "**Table 2** Intermediate formats<br>\n",
        "![Lecture12-7.PNG](https://github.com/TUIlmenauAMS/Videocoding/blob/main/LecturesJupterNotebooks/Lecture12/Img-Lecture12/Lecture12-7.PNG?raw=1)\n",
        "<br>\n",
        "The **Common Interchange Format** (CIF) is a widely used format for digital video. 4CIF has a resolution of 4 times the number of pixels of CIF, and has a resolution similar to standard definition TV.\n",
        "</font></p>    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}